##为什么训练误差比测试误差高很多？
Keras 模型有两种模式：训练和测试。正则化机制，如 Dropout 和 L1/L2 权重正则化，在测试时是关闭的。

此外，训练误差是每批训练数据的平均误差。由于你的模型是随着时间而变化的，一个 epoch 中的第一批数据的误差通常比最后一批的要高。另一方面，测试误差是模型在一个 epoch 训练完后计算的，因而误差较小。
##如何获取中间层的输出？
一个简单的方法是创建一个新的模型来输出你所感兴趣的层

	from keras.models import Model

	model = ...  # 创建原始模型

	layer_name = 'my_layer'
	intermediate_layer_model = Model(inputs=model.input,
												outputs=model.get_layer(layer_name).output)
	intermediate_output = intermediate_layer_model.predict(data)  
如何用 Keras 处理超过内存的数据集？
你可以使用 model.train_on_batch(x，y) 和 model.test_on_batch(x，y) 进行批量训练与测试。请参阅 模型文档。

或者，你可以编写一个生成批处理训练数据的生成器，然后使用  model.fit_generator(data_generator，steps_per_epoch，epochs) 方法。

你可以在 CIFAR10 example 中找到实践代码。

#如何「冻结」网络层？
「冻结」一个层意味着将其排除在训练之外，即其权重将永远不会更新。这在微调模型或使用固定的词向量进行文本输入中很有用。

您可以将 trainable 参数（布尔值）传递给一个层的构造器，以将该层设置为不可训练的：

frozen_layer = Dense(32, trainable=False)
另外，可以在实例化之后将网络层的 trainable 属性设置为 True 或 False。为了使之生效，在修改 trainable 属性之后，需要在模型上调用 compile()。这是一个例子：

	x = Input(shape=(32,))
	layer = Dense(32)
	layer.trainable = False
	y = layer(x)

	frozen_model = Model(x, y)
	# 在下面的模型中，训练期间不会更新层的权重
	frozen_model.compile(optimizer='rmsprop', loss='mse')

	layer.trainable = True
	trainable_model = Model(x, y)
	# 使用这个模型，训练期间 `layer` 的权重将被更新
	# (这也会影响上面的模型，因为它使用了同一个网络层实例)
	trainable_model.compile(optimizer='rmsprop', loss='mse')

	frozen_model.fit(data, labels)  # 这不会更新 `layer` 的权重
	trainable_model.fit(data, labels)  # 这会更新 `layer` 的权重
##如何从 Sequential 模型中移除一个层？
##你可以通过调用 .pop() 来删除 Sequential 模型中最后添加的层：

model = Sequential()
model.add(Dense(32, activation='relu', input_dim=784))
model.add(Dense(32, activation='relu'))

print(len(model.layers))  # "2"

model.pop()
print(len(model.layers))  # "1"
#如何在 Keras 中使用预训练的模型？
我们提供了以下图像分类模型的代码和预训练的权重：

	Xception
	VGG16
	VGG19
	ResNet50
	Inception v3
	Inception-ResNet v2
	MobileNet v1
它们可以使用 keras.applications 模块进行导入：

	from keras.applications.xception import Xception
	from keras.applications.vgg16 import VGG16
	from keras.applications.vgg19 import VGG19
	from keras.applications.resnet50 import ResNet50
	from keras.applications.inception_v3 import InceptionV3
	from keras.applications.inception_resnet_v2 import InceptionResNetV2
	from keras.applications.mobilenet import MobileNet
	
	model = VGG16(weights='imagenet', include_top=True)
#如何在 Keras 中使用 HDF5 输入？
你可以使用 keras.utils.io_utils 中的 HDF5Matrix 类。有关详细信息，请参阅 HDF5Matrix文档。

你也可以直接使用 HDF5 数据集：

import h5py
with h5py.File('input/file.hdf5', 'r') as f:
	x_data = f['x_data']
	model.predict(x_data)
#如何在 Keras 开发过程中获取可复现的结果？
在模型的开发过程中，能够在一次次的运行中获得可复现的结果，以确定性能的变化是来自模型还是数据集的变化，或者仅仅是一些新的随机样本点带来的结果，有时候是很有用处的。下面的代码片段提供了一个如何获得可复现结果的例子 - 针对 Python 3 环境的 TensorFlow 后端。

	import numpy as np
	import tensorflow as tf
	import random as rn

	# 以下是 Python 3.2.3 以上所必需的，
	# 为了使某些基于散列的操作可复现。
	# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED
	# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926

	import os
	os.environ['PYTHONHASHSEED'] = '0'

	# 以下是 Numpy 在一个明确的初始状态生成固定随机数字所必需的。

	np.random.seed(42)

	# 以下是 Python 在一个明确的初始状态生成固定随机数字所必需的。

	rn.seed(12345)

	# 强制 TensorFlow 使用单线程。
	# 多线程是结果不可复现的一个潜在的来源。
	# 更多详情，见: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res

	session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)

	from keras import backend as K

	# `tf.set_random_seed()` 将会以 TensorFlow 为后端，
	# 在一个明确的初始状态下生成固定随机数字。
	# 更多详情，见: https://www.tensorflow.org/api_docs/python/tf/set_random_seed

	tf.set_random_seed(1234)

	sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)
	K.set_session(sess)

	# 剩余代码 ...